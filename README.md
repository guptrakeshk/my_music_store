## Data Engineering 


### A Hands-On, Project-Based instruction for Data Engineering ETL

This is a collection of resources for data engineering ETL for a fictious a music company. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

Email: gupt.rakeshk@gmail.com

This project walks through end-to-end data engineering steps that are needed in a typical project.
Model user activity data to create a database and ETL pipeline in Postgres for a music streaming app. 
Define Fact and Dimension tables and insert data into new tables.

### Steps involved are :
- Creating data models that is needed to cature structured data. Here it is using PostgreSQL as RDBMS.
- Read data resources that are collected from various channels 
- Apply ETL (extract, transform and load) to create pipeline using PostgreSQL.
- Once data is cleansed, transformed and load, it is ready to asnwer queries for analytics.

### Data Modeling and Design steps :
- Model your PostgreSQL database
- Design tables to answer the queries outlined in the project
- Write PostGreSQL CREATE DATABASE , DB Connection and Cursor statements
- Develop your CREATE statement for each of the tables that need to store Factual and Dimension data
- Load the data with INSERT statement for each of the tables
- Include IF NOT EXISTS clauses in your CREATE statements to create tables only if the tables do not already exist.
- Test by running the proper select statements with the correct WHERE clause

### Building ETL Pipeline steps:
Iterate through each event file in event_data to process and create a new CSV file in Python
Generate Apache Cassandra CREATE and INSERT statements to load processed records into relevant tables in your data model
Once data is cleansed, transformed and loaded, it is ready to asnwer specific queries for analytics.
Test by running SELECT statements after running the queries on your database

Here are helpful steps in executing python programs in right sequence. You must execute **create_table.py** first 
in order to create database tables which are needed for storing fact and dimension data.
1. execute `python create_table.py` from CLI or other interface 
2. execute `python etl.py` from CLI or other interface 

#### Additional tips
- One can use `etl.ipynb` Jupyter notebook to try code snippet for validation of logic.
- One can use `test.ipynb` Jupyter notebook to validate data is successfully inserted into tables after ETL.


### Project Dataset 

#### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

### Project artifacts and programs

- `create_tables.py` : A python script to generate database, tables.
- `sql_queries.py` : A python script to store collection of DDL and DML SQL statements.
- `etl.py` : A python script to build an ETL pipeline that involves extract source data, process rawa data and load into tables. 